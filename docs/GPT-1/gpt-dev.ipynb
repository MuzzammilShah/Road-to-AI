{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline language modeling and Code setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Download the dataset](https://github.com/MuzzammilShah/NeuralNetworks-TransformerModel-1/blob/main/cleaned_dataset.txt) from the implementation repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input the dataset and read it in\n",
    "with open('cleaned_dataset.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset (in characters):  6199345\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of dataset (in characters): \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people youd expect to be involved in anything strange or mysterious, because they just didnt hold with such nonsense.\n",
      "\n",
      "Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere.\n",
      "\n",
      "The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didnt think they could bear it if anyone found out about the Potters. Mrs. Potter was Mrs. Dursleys sister, but they hadnt met for several years; \n"
     ]
    }
   ],
   "source": [
    "#The first 1000 characters\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !\"&'()*,-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_abcdefghijklmnopqrstuvwxyz{|}\n",
      "86\n"
     ]
    }
   ],
   "source": [
    "#Listing all the possible unique characters that occur in our dataset\n",
    "characters = sorted(list(set(text)))\n",
    "vocab_size = len(characters)\n",
    "print(''.join(characters))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need some strategy to tokenize the input text. When we say tokenize we mean convert the raw text as a string into some sequence of integers according to some vocabulary of possible elements.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Here in our case we are going to be building a character level language model - So will be translating individual characters into integers.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "We will be implementing encoders and decoders, but rather a simple one (as that should be enough for our usecase).\n",
    "\n",
    "But there are may others (Encoding texts into integers and also decoding them) which use different schema and different vocabularies:\n",
    "\n",
    "- Google uses [sentencepiece](https://github.com/google/sentencepiece): This encoder implements sub-word units. What that means is that it neither considers the entire word nor a single character. And that is what is usually adopted in practice.\n",
    "\n",
    "- OpenAI uses [tiktoken](https://github.com/openai/tiktoken): This uses BPE i.e. Bi Pair Encoding tokenizer and this what GPT uses. Here the vocabulary size is very large, almost upto 50,000 tokens.\n",
    "\n",
    "So here we have tradeoffs:\n",
    "- You can have very long sequence integers with a small vocabulary.\n",
    "- You can have very large vocabulary with a small sequence of integers.\n",
    "\n",
    "Now, we will be sticking to a character level tokenizer only and we are using a simple encoder and decoder. And our vocabulary size is pretty small i.e. `86` characters (so our tradeoff will be that we will have a large sequence of integers when it is encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating mapping from characters to integers\n",
    "\n",
    "stoi = { ch:i for i,ch in enumerate(characters) }\n",
    "itos = { i:ch for i,ch in enumerate(characters) }\n",
    "\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "\n",
    "#Example to see how the encoding and decoding is happening\n",
    "# print(encode(\"harry potter\"))\n",
    "# print(decode(encode(\"harry potter\")))\n",
    "\n",
    "# Output:\n",
    "# [64, 57, 74, 74, 81, 1, 72, 71, 76, 76, 61, 74]\n",
    "# harry potter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6199345]) <built-in method size of Tensor object at 0x000002D0CD42BC40>\n",
      "tensor([38,  1, 74, 11,  1, 57, 70, 60,  1, 38, 74, 75, 11,  1, 29, 77, 74, 75,\n",
      "        68, 61, 81,  9,  1, 71, 62,  1, 70, 77, 69, 58, 61, 74,  1, 62, 71, 77,\n",
      "        74,  9,  1, 41, 74, 65, 78, 61, 76,  1, 29, 74, 65, 78, 61,  9,  1, 79,\n",
      "        61, 74, 61,  1, 72, 74, 71, 77, 60,  1, 76, 71,  1, 75, 57, 81,  1, 76,\n",
      "        64, 57, 76,  1, 76, 64, 61, 81,  1, 79, 61, 74, 61,  1, 72, 61, 74, 62,\n",
      "        61, 59, 76, 68, 81,  1, 70, 71, 74, 69, 57, 68,  9,  1, 76, 64, 57, 70,\n",
      "        67,  1, 81, 71, 77,  1, 78, 61, 74, 81,  1, 69, 77, 59, 64, 11,  1, 45,\n",
      "        64, 61, 81,  1, 79, 61, 74, 61,  1, 76, 64, 61,  1, 68, 57, 75, 76,  1,\n",
      "        72, 61, 71, 72, 68, 61,  1, 81, 71, 77, 60,  1, 61, 80, 72, 61, 59, 76,\n",
      "         1, 76, 71,  1, 58, 61,  1, 65, 70, 78, 71, 68, 78, 61, 60,  1, 65, 70,\n",
      "         1, 57, 70, 81, 76, 64, 65, 70, 63,  1, 75, 76, 74, 57, 70, 63, 61,  1,\n",
      "        71, 74,  1, 69, 81, 75, 76, 61, 74, 65, 71, 77, 75,  9,  1, 58, 61, 59,\n",
      "        57, 77, 75, 61,  1, 76, 64, 61, 81,  1, 66, 77, 75, 76,  1, 60, 65, 60,\n",
      "        70, 76,  1, 64, 71, 68, 60,  1, 79, 65, 76, 64,  1, 75, 77, 59, 64,  1,\n",
      "        70, 71, 70, 75, 61, 70, 75, 61, 11,  0,  0, 38, 74, 11,  1, 29, 77, 74,\n",
      "        75, 68, 61, 81,  1, 79, 57, 75,  1, 76, 64, 61,  1, 60, 65, 74, 61, 59,\n",
      "        76, 71, 74,  1, 71, 62,  1, 57,  1, 62, 65, 74, 69,  1, 59, 57, 68, 68,\n",
      "        61, 60,  1, 32, 74, 77, 70, 70, 65, 70, 63, 75,  9,  1, 79, 64, 65, 59,\n",
      "        64,  1, 69, 57, 60, 61,  1, 60, 74, 65, 68, 68, 75, 11,  1, 33, 61,  1,\n",
      "        79, 57, 75,  1, 57,  1, 58, 65, 63,  9,  1, 58, 61, 61, 62, 81,  1, 69,\n",
      "        57, 70,  1, 79, 65, 76, 64,  1, 64, 57, 74, 60, 68, 81,  1, 57, 70, 81,\n",
      "         1, 70, 61, 59, 67,  9,  1, 57, 68, 76, 64, 71, 77, 63, 64,  1, 64, 61,\n",
      "         1, 60, 65, 60,  1, 64, 57, 78, 61,  1, 57,  1, 78, 61, 74, 81,  1, 68,\n",
      "        57, 74, 63, 61,  1, 69, 77, 75, 76, 57, 59, 64, 61, 11,  1, 38, 74, 75,\n",
      "        11,  1, 29, 77, 74, 75, 68, 61, 81,  1, 79, 57, 75,  1, 76, 64, 65, 70,\n",
      "         1, 57, 70, 60,  1, 58, 68, 71, 70, 60, 61,  1, 57, 70, 60,  1, 64, 57,\n",
      "        60,  1, 70, 61, 57, 74, 68, 81,  1, 76, 79, 65, 59, 61,  1, 76, 64, 61,\n",
      "         1, 77, 75, 77, 57, 68,  1, 57, 69, 71, 77, 70, 76,  1, 71, 62,  1, 70,\n",
      "        61, 59, 67,  9,  1, 79, 64, 65, 59, 64,  1, 59, 57, 69, 61,  1, 65, 70,\n",
      "         1, 78, 61, 74, 81,  1, 77, 75, 61, 62, 77, 68,  1, 57, 75,  1, 75, 64,\n",
      "        61,  1, 75, 72, 61, 70, 76,  1, 75, 71,  1, 69, 77, 59, 64,  1, 71, 62,\n",
      "         1, 64, 61, 74,  1, 76, 65, 69, 61,  1, 59, 74, 57, 70, 65, 70, 63,  1,\n",
      "        71, 78, 61, 74,  1, 63, 57, 74, 60, 61, 70,  1, 62, 61, 70, 59, 61, 75,\n",
      "         9,  1, 75, 72, 81, 65, 70, 63,  1, 71, 70,  1, 76, 64, 61,  1, 70, 61,\n",
      "        65, 63, 64, 58, 71, 74, 75, 11,  1, 45, 64, 61,  1, 29, 77, 74, 75, 68,\n",
      "        61, 81, 75,  1, 64, 57, 60,  1, 57,  1, 75, 69, 57, 68, 68,  1, 75, 71,\n",
      "        70,  1, 59, 57, 68, 68, 61, 60,  1, 29, 77, 60, 68, 61, 81,  1, 57, 70,\n",
      "        60,  1, 65, 70,  1, 76, 64, 61, 65, 74,  1, 71, 72, 65, 70, 65, 71, 70,\n",
      "         1, 76, 64, 61, 74, 61,  1, 79, 57, 75,  1, 70, 71,  1, 62, 65, 70, 61,\n",
      "        74,  1, 58, 71, 81,  1, 57, 70, 81, 79, 64, 61, 74, 61, 11,  0,  0, 45,\n",
      "        64, 61,  1, 29, 77, 74, 75, 68, 61, 81, 75,  1, 64, 57, 60,  1, 61, 78,\n",
      "        61, 74, 81, 76, 64, 65, 70, 63,  1, 76, 64, 61, 81,  1, 79, 57, 70, 76,\n",
      "        61, 60,  9,  1, 58, 77, 76,  1, 76, 64, 61, 81,  1, 57, 68, 75, 71,  1,\n",
      "        64, 57, 60,  1, 57,  1, 75, 61, 59, 74, 61, 76,  9,  1, 57, 70, 60,  1,\n",
      "        76, 64, 61, 65, 74,  1, 63, 74, 61, 57, 76, 61, 75, 76,  1, 62, 61, 57,\n",
      "        74,  1, 79, 57, 75,  1, 76, 64, 57, 76,  1, 75, 71, 69, 61, 58, 71, 60,\n",
      "        81,  1, 79, 71, 77, 68, 60,  1, 60, 65, 75, 59, 71, 78, 61, 74,  1, 65,\n",
      "        76, 11,  1, 45, 64, 61, 81,  1, 60, 65, 60, 70, 76,  1, 76, 64, 65, 70,\n",
      "        67,  1, 76, 64, 61, 81,  1, 59, 71, 77, 68, 60,  1, 58, 61, 57, 74,  1,\n",
      "        65, 76,  1, 65, 62,  1, 57, 70, 81, 71, 70, 61,  1, 62, 71, 77, 70, 60,\n",
      "         1, 71, 77, 76,  1, 57, 58, 71, 77, 76,  1, 76, 64, 61,  1, 41, 71, 76,\n",
      "        76, 61, 74, 75, 11,  1, 38, 74, 75, 11,  1, 41, 71, 76, 76, 61, 74,  1,\n",
      "        79, 57, 75,  1, 38, 74, 75, 11,  1, 29, 77, 74, 75, 68, 61, 81, 75,  1,\n",
      "        75, 65, 75, 76, 61, 74,  9,  1, 58, 77, 76,  1, 76, 64, 61, 81,  1, 64,\n",
      "        57, 60, 70, 76,  1, 69, 61, 76,  1, 62, 71, 74,  1, 75, 61, 78, 61, 74,\n",
      "        57, 68,  1, 81, 61, 57, 74, 75, 24,  1])\n"
     ]
    }
   ],
   "source": [
    "# Now we will be encoding our entire dataset\n",
    "\n",
    "import torch #I used `pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121`my CUDA version is 12.6\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape , data.size)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get to the interesting part (atleast for me lol), we will be splitting the train and validation set. In our case we will be taking 90% for training and remaining for validation. The reason is we dont want our model to completely memorise the dataset and instead generate 'Harry Potter' like texts, hence we are witholding some information and will be using it to check for overfitting at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so now, we never feed our entire data into the model, as that would be computationally expensive and prohibitive. So we divide them into blocks and then group all those blocks into batches and then train them. Each batch is independently trainied and are not communicating with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[79, 57, 68, 67, 65, 70, 63,  1],\n",
      "        [64,  1, 57, 70, 63, 74, 81,  1],\n",
      "        [ 1, 69, 65, 60, 57, 65, 74,  9],\n",
      "        [ 1, 60, 65, 60,  1, 65, 76,  1]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[57, 68, 67, 65, 70, 63,  1, 57],\n",
      "        [ 1, 57, 70, 63, 74, 81,  1, 57],\n",
      "        [69, 65, 60, 57, 65, 74,  9,  1],\n",
      "        [60, 65, 60,  1, 65, 76,  1, 65]])\n",
      "----\n",
      "when input is [79] the target: 57\n",
      "when input is [79, 57] the target: 68\n",
      "when input is [79, 57, 68] the target: 67\n",
      "when input is [79, 57, 68, 67] the target: 65\n",
      "when input is [79, 57, 68, 67, 65] the target: 70\n",
      "when input is [79, 57, 68, 67, 65, 70] the target: 63\n",
      "when input is [79, 57, 68, 67, 65, 70, 63] the target: 1\n",
      "when input is [79, 57, 68, 67, 65, 70, 63, 1] the target: 57\n",
      "when input is [64] the target: 1\n",
      "when input is [64, 1] the target: 57\n",
      "when input is [64, 1, 57] the target: 70\n",
      "when input is [64, 1, 57, 70] the target: 63\n",
      "when input is [64, 1, 57, 70, 63] the target: 74\n",
      "when input is [64, 1, 57, 70, 63, 74] the target: 81\n",
      "when input is [64, 1, 57, 70, 63, 74, 81] the target: 1\n",
      "when input is [64, 1, 57, 70, 63, 74, 81, 1] the target: 57\n",
      "when input is [1] the target: 69\n",
      "when input is [1, 69] the target: 65\n",
      "when input is [1, 69, 65] the target: 60\n",
      "when input is [1, 69, 65, 60] the target: 57\n",
      "when input is [1, 69, 65, 60, 57] the target: 65\n",
      "when input is [1, 69, 65, 60, 57, 65] the target: 74\n",
      "when input is [1, 69, 65, 60, 57, 65, 74] the target: 9\n",
      "when input is [1, 69, 65, 60, 57, 65, 74, 9] the target: 1\n",
      "when input is [1] the target: 60\n",
      "when input is [1, 60] the target: 65\n",
      "when input is [1, 60, 65] the target: 60\n",
      "when input is [1, 60, 65, 60] the target: 1\n",
      "when input is [1, 60, 65, 60, 1] the target: 65\n",
      "when input is [1, 60, 65, 60, 1, 65] the target: 76\n",
      "when input is [1, 60, 65, 60, 1, 65, 76] the target: 1\n",
      "when input is [1, 60, 65, 60, 1, 65, 76, 1] the target: 65\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3007) # My dataset is different from what sensei is using, so i am using my own random number here :)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data #if the function call is for train then it considers train data else the val data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) #this one takes the random chunk of values\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) #x is the first array which will take the values\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]) #y is the second array which will consider the respective target values (\"the next character that needs to be predicted\")\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explaination for above is rather simple, in the first array we have the batch of data which we have considered and each row is the block of data.\n",
    "The second array shows us what the target value will be for the corresponding value in the first array. \n",
    "\n",
    "For example,\n",
    "In first array value is 79 -> so in target array its value will be 57\n",
    "In first array value is 79, 57 -> so in target array its value will be 68\n",
    "and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[79, 57, 68, 67, 65, 70, 63,  1],\n",
      "        [64,  1, 57, 70, 63, 74, 81,  1],\n",
      "        [ 1, 69, 65, 60, 57, 65, 74,  9],\n",
      "        [ 1, 60, 65, 60,  1, 65, 76,  1]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we now have our first set of data which we need to feed into our transformer, we will now implement the simplest model - therefore, the bigram languuage model in pytorch.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Okay so the lecture goes on from the 22nd minute to the 34th, there was a lot of quick breakdown of the code since it was already done in the previous videos (i still couldn't get some of the visuals as to why we did what we did, but lets see how this goes). Also right now, the output maybe a little silly, but apparently we will be needing this generate function in the bigram model class later in the end when we want the model to refer the history of the sentences formed (like if we are at one point in a sentence, then need it to keep track of the previous characters generated up until that point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 86])\n",
      "tensor(4.9531, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "1fzM0Fy_Bufz 1/cPH9mF_c/CYk]kZ573w8,2 \\Oww)(y&9D9,HzR]\n",
      "MOpFbp[&vdr[D9QO4Kl)qKhWCuifZ3YXyi[IK\"\\-8IZdD\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(3007)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so now, we are going to train this above model, we will first declare an optimizer called `AdamW` which is a lot more advance than the one we have been using previously which was the gradient descent optimizer which can also be invoked using `SGD` but that is the simplest one so we are not going ahead with that.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "So at the end of this we are expecting a *slightly*, ***very slightly*** improvements of the output from the above cell (obviously can't expect much because this is a bigram model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.438239336013794\n"
     ]
    }
   ],
   "source": [
    "#Play around with the range value, increase it and see the loss improves overtime, once you see a small enough value (atleast maybe 2.5), stop and check your output in the next cell!\n",
    "\n",
    "batch_size = 32\n",
    "for steps in range(10000): # increase number of steps for good results...\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Th hatI UI, Er oprr wadiomacalse he ffoinele he entich are Nond t eng pasig n the Rokisthe fouthay ry nld Hahabetond IDemeckny yild:Pryo the FIngl, wasioo whed whed id ht crok the r. ofthe stham, whted r monalimind . ll tt menteacile avit be the wand t I Anditan deeveysseishaintained wad hery, g t th cl bres ditiver, herithethened?' nithe NJferrdeng to Du-ce pe t h, awhemin1|jousis. teaby Us. t p e sprusw in ag tt t caunong on, he y It way r herd wa as.\n",
      "\n",
      "' iqutshotadng  hitrstwe che s As Ha\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eh, not bad (jk its gibberish, but hey its forming words in a sentence format yay hahah)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Okay so better explaination on whats happening above:**\n",
    "\n",
    "What we are doing here is that, the tokens are not really talking to each other. \n",
    "\n",
    "Here, although we are passing the entire previous characters, we are only considering the last charcter in that sequence to predict the next one (so now you see why this is a very simple model. So in the above output sentence `Th hatI UI, Er`, for predicting `r` it only considered `E` from the entire sequence of previous characters).\n",
    "*We can see that in the code section `logits = logits[:, -1, :]` where `-1` is added.*\n",
    "\n",
    "Next what we want to do is that we want the tokens to communicate with each other so that there is more context to what needs to be generated next. Therefore allowing us to move into transformers.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "Note: Created a `bigram.py` file which is essentially a python script containing all of the above implemented code. You can run it and see the output produced (There are some modifications to it, where if you have CUDA it utilises it and moves the training inputs, models to it etc. Along with knowing which mode your model is in- whether it is train or val. Apparently this isnt that important for a bigram model, but it is good practise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "### Building the \"self-attention\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
