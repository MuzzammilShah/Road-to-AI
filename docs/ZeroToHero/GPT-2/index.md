# **:material-code-block-tags: TRANSFORMER MODEL - GPT 2**

!!! warning "This project is on a Hiatus due to different ongoing priorities. It's already halfway done, so I'd would love to come back to this soon :)"

**Timeline: 6th April - 25th May, 2025 (On a Hiatus)**

## Introduction

An end-to-end PyTorch implementation of a GPT-2 style language model, based on OpenAI's 124M Paramater model released by OpenAI and Andrej Karpathyâ€™s NanoGPT. The project walks through all major components of the GPT-2 architecture from first principles, including tokenization, positional embeddings, multi-head self-attention, feedforward layers, and transformer blocks. Key ML concepts such as autoregressive language modeling, causal masking, residual connections, and layer normalization are implemented and explained in detail. The final model is trained on real-world text data to generate coherent natural language.