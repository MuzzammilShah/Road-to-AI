{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d23ccf2",
   "metadata": {},
   "source": [
    "# **Section 1 - B**\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d0bf65",
   "metadata": {},
   "source": [
    "## Sample, auto-detect the device\n",
    "\n",
    "At the end of the previous section, we had loaded the pretrained gpt2 model and its weights into our architecture and generated the model. But now we could like to initialize our own weights, we want the model to weights to be generated randomly.\n",
    "\n",
    "So that can be done fairly simple way:\n",
    "\n",
    "```\n",
    "#model = GPT.from_pretrained(\"gpt2\")\n",
    "model = GPT(GPTConfig())\n",
    "```\n",
    "\n",
    "we just call our default `GPTConfig()` that we made. So what PyTorch does is that, it internally assigns random weights to each of the layers in our config, therefore we can use this to generate text from our model.\n",
    "\n",
    "Lastly, before i run this, we also added an additional line of code to better control the device used to run this model. In my case i do have a GPU with CUDA capability. So, if you want to run the model until this point you can also do that using CPU. We have added this additional flag point just to show which device you are using here:\n",
    "\n",
    "```\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "print(f\"Device used: {device})\n",
    "```\n",
    "\n",
    "The rest of the code follows a dynamic approach of detecting the device as well (even in the `forward()` you will see that we have used `device=idx.device`), therefore we are ensuring that all the layers are using the same device while generating.\n",
    "\n",
    "And this is the final output that we generated!\n",
    "\n",
    "![Sampling and Auto detect output](../../../assets/images/gpt2/auto-device-output.png)\n",
    "\n",
    "Obviously it is gibberish lol, we will get to the training next!\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa40857",
   "metadata": {},
   "source": [
    "## Let’s train: data batches (B,T) → logits (B,T,C)\n",
    "\n",
    "> **NOTE**\n",
    ">\n",
    ">We will be loading our dataset now. Sensei used his fav \"The tiny Shakespear dataset\", I am going ahead and using MY Favourite dataset which is what i also used for my GPT-1 implementation which is the HARRY POTTER NOVELS COLLECTION dataset. I directly took the `cleaned_dataset.txt` file which i had processed.\n",
    ">\n",
    ">If you want to see a simple breakdown version of the dataset and what we are about to do, take a look at [this notebook](https://github.com/MuzzammilShah/GPT-TransformerModel-2/blob/main/section-1b-dataset.ipynb) on my repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a764a3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "print(f\"Device used: {device}\")\n",
    "\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "#==========THIS SECTION==========\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "with open('cleaned_dataset.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "data = text[:1000]\n",
    "tokens = enc.encode(data)\n",
    "\n",
    "B, T = 4, 32\n",
    "buf = torch.tensor(tokens[:B*T + 1])\n",
    "x = buf[:-1].view(B, T)\n",
    "y = buf[1:].view(B, T)\n",
    "#================================\n",
    "\n",
    "#model = GPT.from_pretrained(\"gpt2\")\n",
    "model = GPT(GPTConfig())\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "#==========THIS SECTION==========\n",
    "x = x.to(device)\n",
    "logits = model(x)\n",
    "print(logits.shape)\n",
    "import sys; sys.exit(0)\n",
    "#================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c631d1",
   "metadata": {},
   "source": [
    "So the above SECTIONS are the newly added codes just like how they were done in the [dataset breakdown notebook](https://github.com/MuzzammilShah/GPT-TransformerModel-2/blob/main/section-1b-dataset.ipynb), we are only performing a debugging step here therefore the values have been hardcoded. Since we have a batch of 4 by 32, we get the logits for that.\n",
    "\n",
    "- The output we got when the program was run (notice there is a sys exit): **`torch.Size([4, 32, 50257])`**\n",
    "\n",
    "- So `50257` are the logits for what comes next at every position. That is the `x`.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Debugging moment, yay! (Mini version)**\n",
    "\n",
    "So, i had encountered the error `RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!`.\n",
    "\n",
    "This wouldn't happen in sensei's video as he had a manual OVERIDE of the device to cpu. In my case i am continuing to stick with cuda. But since our buffer was initialised manually here: `buf = torch.tensor(tokens[:B*T + 1])`, it by default sits in the cpu. \n",
    "\n",
    "To fix this, we just added one additional line of code: `x = x.to(device)` just before calculating the logits.\n",
    "\n",
    "-----\n",
    "\n",
    "Next we still have the `y` which contains the targets. So now is the time to calculate the loss -> do the backward pass -> and do the optimization. Lets go ahead and calculate the loss first.\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d1cbfc",
   "metadata": {},
   "source": [
    "## Cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26959fcf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#====================\n",
    "x = x.to(device)\n",
    "y = y.to(device)    #newly added\n",
    "\n",
    "# logits = model(x)\n",
    "logits, loss = model(x, y)  #newly added\n",
    "\n",
    "# print(logits.shape)\n",
    "print(loss)     #newly added\n",
    "\n",
    "import sys; sys.exit(0)\n",
    "#===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147c9470",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def forward(self, idx, targets=None): #newly modified\n",
    "\n",
    "        B, T = idx.size()\n",
    "\n",
    "        assert T <= self.config.block_size, f\"Cannot forward this sequence which is of length {T} as the block size is only {self.config.block_size}\"\n",
    "\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
    "        pos_emd = self.transformer.wpe(pos)\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        x = tok_emb + pos_emd\n",
    "\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        loss = None                     #newly added\n",
    "        if targets is not None:         #newly added\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))      #newly added\n",
    "\n",
    "        return logits, loss             #newly modified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa105ae",
   "metadata": {},
   "source": [
    "In the above code snippets, we are calculating the loss and we will do that by modifing the `forward()` function by passing an additional (optional) parameter.\n",
    "\n",
    "So in the function call we do `logits, loss = model(x, y)` and finally in the function we add the new optional parameter `def forward(self, idx, targets=None)`.\n",
    "\n",
    "Then if targets are present (which in our case right now, we do) then we need to calculate the loss and we are using the *cross_entropy* function from PyTorch. So two things are happening here:\n",
    "\n",
    "1. *cross_entropy* can't take multi-dimensional inputs, so in our case, first we have the logits which is in shape (B, T, vocab_size), so we are flattening that out to two-dimension.\n",
    "2. Is the targets, which is in two-dimensional while passing into the function (as we only made it that way), and while calculating the loss that is turned into one-dimension tensors by the *cross_entropy* function.\n",
    "\n",
    "Finally, we get the loss and return the value. (*Note: The same **mini debugging moment** had to be done for `y` as well.*) The output we got is:\n",
    "\n",
    "```\n",
    "Device used: cuda\n",
    "tensor(10.9997, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3041e1f6",
   "metadata": {},
   "source": [
    "> **(IMPORTANT POINTS- Callback to previous videos)**\n",
    ">\n",
    "> The above `loss` value is actually significant. That is basically the value of a single tensor in our vocabulary. \n",
    ">\n",
    "> In the previous videos, we have seen that while initialising networks with random values, we needed to ensure that we get a good starting point. And that is achieved by ensuring that the probabilties of the tokens i.e. the values of the individual tensors are almost uniformly distributed, this way we are not favouring any tokens specifically during initialization.\n",
    ">\n",
    "> So, in our case, we know that our vocab size is 50257. To calculate the even probability for every token we do:\n",
    ">\n",
    "> `1 / 50257` and then we do the loss calculation. Remember that *cross_entropy* essentially does **negative log likelihood**, so when we do that `-ln(1/50257)` we get a value ~10.8 which is actually the loss value we EXPECT at initialization for the respective model.\n",
    ">\n",
    "> Therefore, ours being 10.9 shows us that we are at a good starting point and our probabilty distribution is roughly diffused."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b67cfdb",
   "metadata": {},
   "source": [
    "So at this point we can do a `loss.backward()`, calculate the gradients and then do an optimization!\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c627a0c5",
   "metadata": {},
   "source": [
    "## Optimization loop: overfit a single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4c9d60",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "for i in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Step {i} -> loss value: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ddaf54",
   "metadata": {},
   "source": [
    "Okay so here we are doing optimization on a small set of batch and trying to overfit them. Just as a short term example.\n",
    "\n",
    "- We first declare the optimizer object using pytorch and we use the `AdamW` optimizer. We used to use Stochastic Gradient one and then Adam is also used, but it seems `AdamW` is a bug fix version of Adam, so we are going ahead with that. We don't really have to know its workings to the depth, for now we will treat this as a black box itself. Ultimately, its aim is to speed up this whole optimization process.\n",
    "\n",
    "- We are passing the model parameters to it and we are setting the learning rate to 3 to the power negative four as that is like the most common or best value to set for almost any model.\n",
    "\n",
    "- Next we are creating a small loop over a small batch, our case 50.\n",
    "\n",
    "- First step as a standard rule we have to start by zeroing the gradients (we saw this in the first lecture).\n",
    "\n",
    "- Following are the usual steps of passing the inputs and targets to calculate the logits and loss, then doing the backward operation on loss and finally there is a `optimizer.step()` which is used to increament the optimizer to the next step i.e. the next batch.\n",
    "\n",
    "- Just one additional point while printing where we are using `loss.item()`, the `loss` is a single value tensor, so by adding `.item()` what PyTorch is doing internally is that it takes that value and stores it as a float in our device."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec5f83d",
   "metadata": {},
   "source": [
    "### **Debugging moment, yay! (Mini version - update)**\n",
    "\n",
    "So just one fix, sensei had also seemed to encounter the same error (and obviously he had a smarter fix to it), we had to move the buffer to the device. And turns out we can't just directly add `.to(device)` as unlike in `model` the buffer `buf` would just point it to another memory within the same device and not convert it into the device itself we expect it should. Therefore we assign it to update it like: `buf = buf.to(device)`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2701787f",
   "metadata": {},
   "source": [
    "Finally, we ran it and we got some pretty decent loss values for this small batch:\n",
    "\n",
    "<div style=\"display: flex; gap: 10px;\">\n",
    "  <img src=\"../../../assets/images/gpt2/optimization-overfit-1.png\" alt=\"Optimizer output 1\" style=\"width: 48%;\">\n",
    "  <img src=\"../../../assets/images/gpt2/optimization-overfit-2.png\" alt=\"Optimizer output 2\" style=\"width: 48%;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70da225",
   "metadata": {},
   "source": [
    "So what we did here was to overfit the model to just this batch of 50, therefore it like memorises it to the core. Now that this is done, what we actually want to do is to optimize this model as a whole. We are going to iterate the `x` and `y` values, while also create a small data loader which ensures that we keep getting fresh batches and that we are optimizing it well.\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a6ea74",
   "metadata": {},
   "source": [
    "## Data loader lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b963f212",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class DataLoaderLite:\n",
    "\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        with open('cleaned_dataset.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "\n",
    "        print(f\"Loaded {len(self.tokens)} tokens\")\n",
    "        print(f\"One epoch will process {len(self.tokens) // (B*T)} batches\")\n",
    "\n",
    "        self.current_postion = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_postion : self.current_postion+B*T+1]\n",
    "        x = (buf[:-1]).view(B, T)\n",
    "        y = (buf[1:]).view(B, T)\n",
    "\n",
    "        self.current_postion += B*T\n",
    "\n",
    "        if self.current_postion + (B*T+1) > len(self.tokens):\n",
    "            self.current_postion = 0\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5316cd6a",
   "metadata": {},
   "source": [
    "We are implementing one simple data loader here that simply goes through the file containing the data in chunks.\n",
    "\n",
    "In the `__init__` method we are reading the dataset, tokenizing it, encoding it and converting it to tensors. Finally printing how many batches get iterated in one epoch. And we set the *current_position* to 0.\n",
    "\n",
    "In the `next_batch` method we are processing it in batches B times T. So we are loading it in batches of (B, T). So it is those chunks of batches we are processsing at the time. Therefore, even the *current_position* is being increated my B times T. \n",
    "\n",
    "The buffers and setting of x, y (of setting inputs and tagets tensors) is the same as what we did in [section1b dataset notebook](https://github.com/MuzzammilShah/GPT-TransformerModel-2/blob/main/section-1b-dataset.ipynb).\n",
    "\n",
    "Lastly, once we run out of data we just reset the *current_position* to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac72f0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoaderLite(B=4, T=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f4a6e6",
   "metadata": {},
   "source": [
    "Here, we removed the entire '#THIS SECTION' code we saw at the start of this section and just set the DataLoader, with the same parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9fd09c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "for i in range(50):\n",
    "    x, y = train_loader.next_batch()    #newly added\n",
    "    x, y = x.to(device), y.to(device)   #newly added\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Step {i} -> loss value: {loss.item()}\")\n",
    "\n",
    "import sys; sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e71df4",
   "metadata": {},
   "source": [
    "Finally, we call the `next_batch` on x and y; and importantly move them to GPU (As by default our DataLoaderLite class processes it in CPU. Because, when we converted the input text to tokens in there, we didnt move the tokens to GPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dba00c",
   "metadata": {},
   "source": [
    "So, we ran our code to see what the loss is like now.\n",
    "\n",
    "What's different this time, is that, just in the previous step, we only kept training on a single batch, so overfitting occured and got like a very very low loss.\n",
    "\n",
    "This time we are adding different batches. So we expect to see a different loss value at the end. Now, although it wont be as low as before, we still expect it to be less than out estimated value, 11. \n",
    "\n",
    "This is because, as the model learns from the data, it will notice that a lot of the tokens wont be repeated, like unicodes, special characters, complex words etc. So a lot of those will be filtered out.\n",
    "\n",
    "But ofcourse, the ultimate aim is to get the loss close to 0, that is when our model is perfect. Since right now, we are doing only like 50 iterations, we don't expect it to go that low as seen in the output we recieved.\n",
    "\n",
    "<div style=\"display: flex; gap: 10px;\">\n",
    "  <img src=\"../../../assets/images/gpt2/data-loader-1.png\" alt=\"Optimizer output 1\" style=\"width: 48%;\">\n",
    "  <img src=\"../../../assets/images/gpt2/data-loader-2.png\" alt=\"Optimizer output 2\" style=\"width: 48%;\">\n",
    "</div>\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a2cbcb",
   "metadata": {},
   "source": [
    "## Parameter sharing wte and lm_head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711edb38",
   "metadata": {},
   "source": [
    "The general idea (atleast how much i understood it)-\n",
    "\n",
    "The tokens considered during the input and the ones produced during the output, you would expect that the tokens which have similar semantics, for example: Tokens in lower case vs in upper case, tokens with same word but in different languages; you would expect them to be near in the semantics graph. Similaryly, if it is the exact same tokens, then you would expect them to have the same weights. Therefore, at the start and end of the transformer model, there is this common property which is implemented where the similar tokens are mapped together- So, \"Similar tokens should have similar properties/similar embeddings/similar weights\".\n",
    "\n",
    "Ultimately, what the researches found is that, the output embeddings are very similar to word embeddings (at the input), so they tried to tie them together and realised that they get much better outputs. This was whats done in the Attention is all you need paper and also what OpenAI did in their implementation of gpt2. Naturally, it is what we are going to implement as well :)\n",
    "\n",
    "We only added this line of code in the end of the init method of our GPT module:\n",
    "\n",
    "\n",
    "```\n",
    "self.transformer.wte.weight = self.lm_head.weight   #weight sharing scheme\n",
    "```\n",
    "\n",
    "So we have implemented a weight tieing scheme and we end up saving a lot of parameters. I am still not completely sure how this effects our model, because in my case i ended up getting a higher loss value than before, but we'll see where this takes us.\n",
    "\n",
    "<div style=\"display: flex; gap: 10px;\">\n",
    "  <img src=\"../../../assets/images/gpt2/parameter-sharing-1.png\" alt=\"Optimizer output 1\" style=\"width: 48%;\">\n",
    "  <img src=\"../../../assets/images/gpt2/parameter-sharing-2.png\" alt=\"Optimizer output 2\" style=\"width: 48%;\">\n",
    "</div>\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0e5876",
   "metadata": {},
   "source": [
    "## Model initialization: std 0.02, residual init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c42329d",
   "metadata": {},
   "source": [
    "Here, we are focusing on getting our initializations right as well to match with the implementation of GPT 2. There are two major updates: Initialization of standard deviation to 0.02 and Weights of the Residual layer at initialization.\n",
    "\n",
    "**Update 1:**\n",
    "\n",
    "Although the details are not mentioned explicitly, we can read between the lines in the code and find out what they have done. The notable ones would be the standard deviation to be 0.02 especially for the linear layer and token embeddings, 0.01 for positional embeddings and bias being zero. That is exactly what we will implement (except in ours, both the positional and token embeddings will be 0.02).\n",
    "\n",
    "The custom code written by sensei:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d7bdd8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def _init_weights(self, module):\n",
    "\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f835a142",
   "metadata": {},
   "source": [
    "funtion call at the end of init function just above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bef2fd2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "self.apply(self._init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fa0ef5",
   "metadata": {},
   "source": [
    "Apart from this, in our implementation, the only other layer that has initialization and has parameters is the `Layer Norm` in the MLP Module. But the PyTorch default initialization sets the *scale* to be *0* and *offset* to be *1*, and that is exactly what we want.\n",
    "\n",
    "Finally, it is important to know that, normally we would have the standard deviation as some dynamic value that would increase with the number of parameters, but here we are strictly going with what GPT 2 did, so the set value of 0.02 is added by us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2ce22a",
   "metadata": {},
   "source": [
    "**Update 2:**\n",
    "\n",
    "This is to control the growth of activations inside the **residual stream** in the forward pass. Now, the residual stream is as we saw during our implementation, the addition of the values as it goes up the layer and we would notice that for us those values keep on increasing. Therefore if that value is N, the paper suggests that we do a 1 to the squareroot of N. So, 1 to the square root of the number of residual layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6854ad4",
   "metadata": {},
   "source": [
    "So we set the following 'flag' after the `c_proj` in the MLP and CasualSelfAttention layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc924e44",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "self.c_proj.NANOGPT_SCALE_INIT = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc75f7b0",
   "metadata": {},
   "source": [
    "Then we add the calculation in the GPT module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3364ba6f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def _init_weights(self, module):\n",
    "\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02  #here\n",
    "            if hasattr(module, 'NANOGPT_SCALE_INIT'):   #here\n",
    "                std *= (2 * self.config.n_layer) ** -0.5    #here\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b719dba",
   "metadata": {},
   "source": [
    "The `(2 * self.config.n_layer)` is because we have two types of layers in a block i.e. Attention and MLP.\n",
    "\n",
    "Here's how it performed (although i am a little concerned at this point lmao the loss is NOT getting better...?)\n",
    "\n",
    "<div style=\"display: flex; gap: 10px;\">\n",
    "  <img src=\"../../../assets/images/gpt2/initialisation-1.png\" alt=\"Optimizer output 1\" style=\"width: 48%;\">\n",
    "  <img src=\"../../../assets/images/gpt2/initialisation-2.png\" alt=\"Optimizer output 2\" style=\"width: 48%;\">\n",
    "</div>\n",
    "\n",
    "And with that, we have the initialization which is the same as in GPT 2!\n",
    "\n",
    "&nbsp;"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
